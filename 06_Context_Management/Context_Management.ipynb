{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **🔹 Context Management**"
      ],
      "metadata": {
        "id": "Jx-thu2w8N6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🔸Local Context**\n",
        "Bank Account Example"
      ],
      "metadata": {
        "id": "1AGzoZiMsxRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-agents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Z5BbQ6W5s7EN",
        "outputId": "53780579-7239-4457-de49-6ac3181efd34"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-agents\n",
            "  Downloading openai_agents-0.2.8-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting griffe<2,>=1.5.6 (from openai-agents)\n",
            "  Downloading griffe-1.12.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting mcp<2,>=1.11.0 (from openai-agents)\n",
            "  Downloading mcp-1.13.0-py3-none-any.whl.metadata (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openai<2,>=1.99.6 in /usr/local/lib/python3.11/dist-packages (from openai-agents) (1.99.9)\n",
            "Requirement already satisfied: pydantic<3,>=2.10 in /usr/local/lib/python3.11/dist-packages (from openai-agents) (2.11.7)\n",
            "Requirement already satisfied: requests<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from openai-agents) (2.32.3)\n",
            "Collecting types-requests<3,>=2.0 (from openai-agents)\n",
            "  Downloading types_requests-2.32.4.20250809-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from openai-agents) (4.14.1)\n",
            "Collecting colorama>=0.4 (from griffe<2,>=1.5.6->openai-agents)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio>=4.5 in /usr/local/lib/python3.11/dist-packages (from mcp<2,>=1.11.0->openai-agents) (4.10.0)\n",
            "Collecting httpx-sse>=0.4 (from mcp<2,>=1.11.0->openai-agents)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: httpx>=0.27.1 in /usr/local/lib/python3.11/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.28.1)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.11/dist-packages (from mcp<2,>=1.11.0->openai-agents) (4.25.0)\n",
            "Collecting pydantic-settings>=2.5.2 (from mcp<2,>=1.11.0->openai-agents)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.0.20)\n",
            "Collecting sse-starlette>=1.6.1 (from mcp<2,>=1.11.0->openai-agents)\n",
            "  Downloading sse_starlette-3.0.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.11/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.47.2)\n",
            "Requirement already satisfied: uvicorn>=0.31.1 in /usr/local/lib/python3.11/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.35.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2,>=1.99.6->openai-agents) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2,>=1.99.6->openai-agents) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2,>=1.99.6->openai-agents) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2,>=1.99.6->openai-agents) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.10->openai-agents) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.10->openai-agents) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.10->openai-agents) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0->openai-agents) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0->openai-agents) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0->openai-agents) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0->openai-agents) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.1->mcp<2,>=1.11.0->openai-agents) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.1->mcp<2,>=1.11.0->openai-agents) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (0.27.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings>=2.5.2->mcp<2,>=1.11.0->openai-agents)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.31.1->mcp<2,>=1.11.0->openai-agents) (8.2.1)\n",
            "Downloading openai_agents-0.2.8-py3-none-any.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.5/168.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading griffe-1.12.1-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mcp-1.13.0-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.2/160.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.4.20250809-py3-none-any.whl (20 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-3.0.2-py3-none-any.whl (11 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: types-requests, python-dotenv, httpx-sse, colorama, sse-starlette, griffe, pydantic-settings, mcp, openai-agents\n",
            "Successfully installed colorama-0.4.6 griffe-1.12.1 httpx-sse-0.4.1 mcp-1.13.0 openai-agents-0.2.8 pydantic-settings-2.10.1 python-dotenv-1.1.1 sse-starlette-3.0.2 types-requests-2.32.4.20250809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Tdp1Lztks_0m",
        "outputId": "319100b8-1248-46e7-f808-948ee6572c4d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner, RunConfig, OpenAIChatCompletionsModel, AsyncOpenAI, RunContextWrapper, function_tool\n",
        "import nest_asyncio\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "nest_asyncio.apply()\n",
        "gemini_api_key = \"AIzaSyAzjfiUtoEpGMp1hjSlkhfEakL6eoQCkec\"\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=client\n",
        ")\n",
        "\n",
        "config = RunConfig(\n",
        "    model=model,\n",
        "    model_provider=client,\n",
        "    tracing_disabled=True\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class AccountHolders:\n",
        "    name: str\n",
        "    age: int\n",
        "    balance: str\n",
        "    password: str\n",
        "    uid: str\n",
        "\n",
        "@dataclass\n",
        "class AccountsDB:\n",
        "    accounts: list[AccountHolders] = field(default_factory=list)\n",
        "\n",
        "@function_tool\n",
        "def get_account_holder_info(wrapper: RunContextWrapper[AccountsDB], password: str = None, uid: str = None, name: str = None):\n",
        "    \"\"\"\n",
        "    Fetch the user account information.\n",
        "    - If user does not provide password or uid → ask for password or uid.\n",
        "    - If wrong password → tell user and ask again.\n",
        "    - If correct password → return account data.\n",
        "    \"\"\"\n",
        "\n",
        "    accounts = wrapper.context.accounts\n",
        "\n",
        "    if name is None:\n",
        "        return \"Please provide the account holder name.\"\n",
        "\n",
        "    # search by name\n",
        "    for acc in accounts:\n",
        "        if acc.name.lower() == name.lower():\n",
        "            if uid is None:\n",
        "                return f\"Please provide the uid for account holder {acc.name}.\"\n",
        "            if uid != acc.uid:\n",
        "                return f\"Incorrect uid for {acc.name}. Please try again.\"\n",
        "\n",
        "            if password is None:\n",
        "                return f\"Please provide the password for account holder {acc.name} (uid: {acc.uid}).\"\n",
        "            if password != acc.password:\n",
        "                return f\"Incorrect password for {acc.name}. Please try again.\"\n",
        "\n",
        "            return f\"Account Holder: {acc.name}, Age: {acc.age}, Balance: {acc.balance}\"\n",
        "\n",
        "    return \"No account holder found with that name.\"\n",
        "\n",
        "agent: Agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=(\n",
        "        \"You are a helpful assistant in a **banking simulation system**. \"\n",
        "        \"The account data provided is purely DUMMY and not real. \"\n",
        "        \"You are allowed to fetch and show this data, but only when the correct UID and password are provided. \"\n",
        "        \"If either UID or password is missing, ask the user for it. \"\n",
        "        \"If incorrect, tell them it's wrong and ask again.\"\n",
        "    ),\n",
        "    tools=[get_account_holder_info],\n",
        ")\n",
        "\n",
        "async def main():\n",
        "  mesum = AccountHolders(name=\"Mesum\", age=19, balance=\"Rs.2500000\", password=\"account01\",  uid=\"uid001\")\n",
        "  ali = AccountHolders(name=\"Ali\", age=29, balance=\"Rs.5000\", password=\"account02\",  uid=\"uid002\")\n",
        "  zain = AccountHolders(name=\"Zain\", age=24, balance=\"Rs.200\", password=\"account03\",  uid=\"uid003\")\n",
        "\n",
        "  db = AccountsDB(accounts=[mesum, ali, zain])\n",
        "\n",
        "  result = await Runner.run(\n",
        "      starting_agent=agent,\n",
        "      input=\"What is the account balance of Ali? password=account02 uid=uid002\",\n",
        "      run_config=config,\n",
        "      context=db\n",
        "      )\n",
        "\n",
        "  print(result.final_output)\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqN98fFhtYbj",
        "outputId": "133f1a5a-5b4f-4733-bac6-5ffac4fa4484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK. The account balance of Ali is Rs.5000.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🔸LLM Context**\n",
        "**Instructions** Example => 01: static string ky sath instruction"
      ],
      "metadata": {
        "id": "qhCU9usz6HFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner, RunConfig, OpenAIChatCompletionsModel, AsyncOpenAI, RunContextWrapper, function_tool\n",
        "import nest_asyncio\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "nest_asyncio.apply()\n",
        "gemini_api_key = \"AIzaSyAzjfiUtoEpGMp1hjSlkhfEakL6eoQCkec\"\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=client\n",
        ")\n",
        "\n",
        "config = RunConfig(\n",
        "    model=model,\n",
        "    model_provider=client,\n",
        "    tracing_disabled=True\n",
        ")\n",
        "\n",
        "agent: Agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"\"\"\n",
        "          You are a helpful assistant.\n",
        "          user name is Mesum.\n",
        "          user is like ans in roman urdu.\n",
        "          Always this remember this information and using the generate the answer.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "async def main():\n",
        "  result = await Runner.run(\n",
        "      starting_agent=agent,\n",
        "      input=\"What is the name of user\",\n",
        "      run_config=config,\n",
        "      )\n",
        "\n",
        "  print(result.final_output)\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPtZm9QC68Uh",
        "outputId": "7eb2e312-1b4b-4a7b-8004-d3272be8b14c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User ka naam Mesum hai.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🔸LLM Context**\n",
        "**Instructions** Example => 02: dynamic function ky sath instruction jo context recive kary"
      ],
      "metadata": {
        "id": "w1vMQll9-SUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner, RunConfig, OpenAIChatCompletionsModel, AsyncOpenAI, RunContextWrapper, function_tool\n",
        "import nest_asyncio\n",
        "from datetime import datetime\n",
        "\n",
        "nest_asyncio.apply()\n",
        "gemini_api_key = \"AIzaSyAzjfiUtoEpGMp1hjSlkhfEakL6eoQCkec\"\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=client\n",
        ")\n",
        "\n",
        "config = RunConfig(\n",
        "    model=model,\n",
        "    model_provider=client,\n",
        "    tracing_disabled=True\n",
        ")\n",
        "\n",
        "def get_dynamic_instructions(user_name: str, preferred_language):\n",
        "  today = datetime.now().strftime(\"%d-%m-%y\")\n",
        "  instructions = f\"\"\"\n",
        "          You are a helpful assistant.\n",
        "          user name is {user_name}.\n",
        "          user is like ans in {preferred_language}.\n",
        "          Always this remember this information and using the generate the answer.\n",
        "          Today is {today}\n",
        "        \"\"\"\n",
        "  return instructions\n",
        "\n",
        "agent: Agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=get_dynamic_instructions(\"Mesum\", \"roman urdu\")\n",
        "    )\n",
        "\n",
        "async def main():\n",
        "  result = await Runner.run(\n",
        "      starting_agent=agent,\n",
        "      input=\"Hello\",\n",
        "      run_config=config,\n",
        "      )\n",
        "\n",
        "  print(result.final_output)\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBneW9VP-Rs2",
        "outputId": "c5fb8f61-1f17-49c4-ba8f-42af307ccfcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Mesum! Kya haal hai?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🔸LLM Context**\n",
        "**Input** Example ky sath."
      ],
      "metadata": {
        "id": "8Rrv4ZXpAGm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner, RunConfig, OpenAIChatCompletionsModel, AsyncOpenAI\n",
        "import nest_asyncio\n",
        "from datetime import datetime\n",
        "\n",
        "nest_asyncio.apply()\n",
        "gemini_api_key = \"AIzaSyAzjfiUtoEpGMp1hjSlkhfEakL6eoQCkec\"\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=client\n",
        ")\n",
        "\n",
        "config = RunConfig(\n",
        "    model=model,\n",
        "    model_provider=client,\n",
        "    tracing_disabled=True\n",
        ")\n",
        "\n",
        "agent: Agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are a helpful HR assistant.\",\n",
        "    )\n",
        "\n",
        "async def main():\n",
        "  result = await Runner.run(\n",
        "      starting_agent=agent,\n",
        "      input=[\n",
        "        {\"role\": \"assistant\", \"content\": \"Employee record: Ali is a Software Engineer.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Employee record: Ali's salary is 250,000 PKR per month.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me the salary of Ali.\"}\n",
        "    ],\n",
        "      run_config=config,\n",
        "      )\n",
        "\n",
        "  print(result.final_output)\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "id": "e69vo8nyASvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57dff85a-83d3-4c8b-a0a1-d43e83f9df65"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ali's salary is 250,000 PKR per month.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🔸LLM Context**\n",
        "**Fetch Data** Example ky sath."
      ],
      "metadata": {
        "id": "5PMdJcWRDbuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner, RunConfig, OpenAIChatCompletionsModel, AsyncOpenAI, function_tool\n",
        "import nest_asyncio\n",
        "from datetime import datetime\n",
        "\n",
        "nest_asyncio.apply()\n",
        "gemini_api_key = \"AIzaSyAzjfiUtoEpGMp1hjSlkhfEakL6eoQCkec\"\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=client\n",
        ")\n",
        "\n",
        "config = RunConfig(\n",
        "    model=model,\n",
        "    model_provider=client,\n",
        "    tracing_disabled=True\n",
        ")\n",
        "\n",
        "employee_db = {\n",
        "    \"mesum\": {\"preferred_language\": \"Roman urdu\", \"role\": \"Software Engineer\"}\n",
        "}\n",
        "\n",
        "@function_tool\n",
        "def userData(name: str):\n",
        "  \"\"\"\n",
        "  This is a user data when user ask the question use this information and generate the response.\n",
        "\n",
        "  Args:\n",
        "    name: Name of the user.\n",
        "    preferred_language: Preferred language of the user.\n",
        "    role: Role of the user.\n",
        "  \"\"\"\n",
        "  user = employee_db.get(name.lower())\n",
        "  if user is None:\n",
        "    return \"User not found.\"\n",
        "\n",
        "  return f\"Name of user is {name}, user preferred language is {user['preferred_language']} and user is a {user['role']}.\"\n",
        "\n",
        "agent: Agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are a helpful HR assistant.\",\n",
        "    tools=[userData]\n",
        "    )\n",
        "\n",
        "async def main():\n",
        "  result = await Runner.run(\n",
        "      starting_agent=agent,\n",
        "      input=\"Hello, What is Mesum profile.\",\n",
        "      run_config=config,\n",
        "      )\n",
        "\n",
        "  print(result.final_output)\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTQEVZndDmnT",
        "outputId": "771cb972-f291-4156-f30a-530f2858adff"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mesum is a Software Engineer and prefers to communicate in Roman Urdu.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}